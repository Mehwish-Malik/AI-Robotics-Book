"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[759],{7274:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var t=i(4848),r=i(8453);const s={sidebar_position:1},a="Chapter 10: Computer Vision and Perception",o={id:"modules/module-4-ai/chapter-10-perception",title:"Chapter 10: Computer Vision and Perception",description:"Summary",source:"@site/docs/modules/module-4-ai/chapter-10-perception.md",sourceDirName:"modules/module-4-ai",slug:"/modules/module-4-ai/chapter-10-perception",permalink:"/docs/modules/module-4-ai/chapter-10-perception",draft:!1,unlisted:!1,editUrl:"https://github.com/Mehwish-Malik/AI-Robotics-Book.git/docs/modules/module-4-ai/chapter-10-perception.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Chapter 9: Balance and Stability Control",permalink:"/docs/modules/module-3-control/chapter-9-balance"},next:{title:"Chapter 11: Machine Learning for Robotics",permalink:"/docs/modules/module-4-ai/chapter-11-learning"}},l={},c=[{value:"Summary",id:"summary",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Introduction to Computer Vision in Robotics",id:"introduction-to-computer-vision-in-robotics",level:2},{value:"Vision Requirements in Robotics",id:"vision-requirements-in-robotics",level:3},{value:"Vision System Architecture",id:"vision-system-architecture",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Traditional Approaches",id:"traditional-approaches",level:3},{value:"Deep Learning Approaches",id:"deep-learning-approaches",level:3},{value:"Recognition Pipelines",id:"recognition-pipelines",level:3},{value:"3D Object Recognition",id:"3d-object-recognition",level:3},{value:"Scene Understanding",id:"scene-understanding",level:2},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:3},{value:"Panoptic Segmentation",id:"panoptic-segmentation",level:3},{value:"Depth Estimation",id:"depth-estimation",level:3},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Definition and Applications",id:"definition-and-applications",level:3},{value:"Image-Based Visual Servoing (IBVS)",id:"image-based-visual-servoing-ibvs",level:3},{value:"Position-Based Visual Servoing (PBVS)",id:"position-based-visual-servoing-pbvs",level:3},{value:"Hybrid Approaches",id:"hybrid-approaches",level:3},{value:"SLAM (Simultaneous Localization and Mapping)",id:"slam-simultaneous-localization-and-mapping",level:2},{value:"SLAM Fundamentals",id:"slam-fundamentals",level:3},{value:"Feature-based SLAM",id:"feature-based-slam",level:3},{value:"Direct SLAM",id:"direct-slam",level:3},{value:"Deep Learning SLAM",id:"deep-learning-slam",level:3},{value:"Multi-modal Perception",id:"multi-modal-perception",level:2},{value:"Sensor Fusion with Vision",id:"sensor-fusion-with-vision",level:3},{value:"Cross-modal Learning",id:"cross-modal-learning",level:3},{value:"Technical Depth: Mathematical Foundations",id:"technical-depth-mathematical-foundations",level:2},{value:"Camera Models",id:"camera-models",level:3},{value:"Epipolar Geometry",id:"epipolar-geometry",level:3},{value:"Visual Jacobian",id:"visual-jacobian",level:3},{value:"Practical Applications",id:"practical-applications",level:2},{value:"Navigation and Mapping",id:"navigation-and-mapping",level:3},{value:"Manipulation and Grasping",id:"manipulation-and-grasping",level:3},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:3},{value:"Challenges",id:"challenges",level:2},{value:"Real-time Processing",id:"real-time-processing",level:3},{value:"Environmental Variability",id:"environmental-variability",level:3},{value:"Scale and Resource Constraints",id:"scale-and-resource-constraints",level:3},{value:"Integration Complexity",id:"integration-complexity",level:3},{value:"Figure List",id:"figure-list",level:2},{value:"Code Example: Computer Vision Implementation",id:"code-example-computer-vision-implementation",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary-1",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-10-computer-vision-and-perception",children:"Chapter 10: Computer Vision and Perception"}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter explores the critical role of computer vision in humanoid robotics, focusing on how robots perceive and understand their visual environment. We'll examine object recognition, scene understanding, visual servoing, SLAM systems, and the integration of vision with other sensory modalities. Understanding computer vision is essential for creating robots that can operate autonomously in complex, visually-rich environments."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the fundamentals of computer vision for robotics applications"}),"\n",(0,t.jsx)(n.li,{children:"Implement object detection and recognition algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Apply SLAM (Simultaneous Localization and Mapping) techniques"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision with other sensory modalities"}),"\n",(0,t.jsx)(n.li,{children:"Design visual servoing systems for robot control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computer Vision"}),": Extraction of information from visual data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": Locating and identifying objects in images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scene Understanding"}),": Interpreting environmental context"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Servoing"}),": Controlling robot motion based on visual feedback"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-modal Perception"}),": Integration of vision with other sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Processing visual data within time constraints"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-computer-vision-in-robotics",children:"Introduction to Computer Vision in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision enables humanoid robots to perceive and interpret their visual environment, which is crucial for navigation, manipulation, and interaction. Unlike traditional computer vision applications, robotics vision must operate in real-time with uncertain conditions and must integrate with other systems for complete environmental understanding."}),"\n",(0,t.jsx)(n.h3,{id:"vision-requirements-in-robotics",children:"Vision Requirements in Robotics"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Visual processing must keep up with robot motion and control rates\n",(0,t.jsx)(n.strong,{children:"Robustness"}),": Systems must handle varying lighting, occlusions, and environmental conditions\n",(0,t.jsx)(n.strong,{children:"Accuracy"}),": Precise measurements required for manipulation and navigation\n",(0,t.jsx)(n.strong,{children:"Integration"}),": Vision data must integrate with other sensors and control systems\n",(0,t.jsx)(n.strong,{children:"Efficiency"}),": Limited computational resources on robot platforms"]}),"\n",(0,t.jsx)(n.h3,{id:"vision-system-architecture",children:"Vision System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Image Acquisition \u2192 Preprocessing \u2192 Feature Extraction \u2192 Recognition \u2192 Action\n"})}),"\n",(0,t.jsx)(n.p,{children:"Each stage must be optimized for the robot's specific requirements and constraints."}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"traditional-approaches",children:"Traditional Approaches"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Template Matching"}),": Compare image patches with stored templates"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simple to implement"}),"\n",(0,t.jsx)(n.li,{children:"Sensitive to scale, rotation, and lighting changes"}),"\n",(0,t.jsx)(n.li,{children:"Computationally expensive for large template sets"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Feature-based Methods"}),": Extract distinctive features and match them"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"SIFT (Scale-Invariant Feature Transform)"}),"\n",(0,t.jsx)(n.li,{children:"SURF (Speeded Up Robust Features)"}),"\n",(0,t.jsx)(n.li,{children:"HOG (Histogram of Oriented Gradients)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-approaches",children:"Deep Learning Approaches"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Convolutional Neural Networks (CNNs)"}),": Hierarchical feature extraction"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Input Image \u2192 Conv Layers \u2192 Pooling \u2192 Fully Connected \u2192 Output\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection Networks"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"YOLO (You Only Look Once)"}),": Single-pass detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SSD (Single Shot Detector)"}),": Multi-scale feature maps"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"R-CNN Family"}),": Region-based detection with proposal generation"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"recognition-pipelines",children:"Recognition Pipelines"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Two-Stage Pipeline"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Region Proposal: Identify potential object locations"}),"\n",(0,t.jsx)(n.li,{children:"Classification: Classify proposed regions"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Single-Stage Pipeline"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Direct prediction of bounding boxes and classes"}),"\n",(0,t.jsx)(n.li,{children:"Faster but potentially less accurate"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3d-object-recognition",children:"3D Object Recognition"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multi-view Recognition"}),": Combine information from multiple viewpoints\n",(0,t.jsx)(n.strong,{children:"Shape-based Recognition"}),": Use 3D shape models for recognition\n",(0,t.jsx)(n.strong,{children:"Deep Learning on 3D Data"}),": Point clouds, meshes, and voxel grids"]}),"\n",(0,t.jsx)(n.h2,{id:"scene-understanding",children:"Scene Understanding"}),"\n",(0,t.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Assigning semantic labels to each pixel in an image:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"L = argmax_l \u03a3_i P(label_i = l | image, position_i)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FCN (Fully Convolutional Networks)"}),": End-to-end segmentation\n",(0,t.jsx)(n.strong,{children:"U-Net"}),": Encoder-decoder with skip connections\n",(0,t.jsx)(n.strong,{children:"DeepLab"}),": Atrous convolutions for multi-scale context"]}),"\n",(0,t.jsx)(n.h3,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Distinguishing individual object instances:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mask R-CNN"}),": Extends object detection with segmentation masks\n",(0,t.jsx)(n.strong,{children:"YOLACT"}),": Real-time instance segmentation\n",(0,t.jsx)(n.strong,{children:"PolarMask"}),": Instance segmentation as instance classification"]}),"\n",(0,t.jsx)(n.h3,{id:"panoptic-segmentation",children:"Panoptic Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Combining semantic and instance segmentation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Thing classes"}),": Countable objects (people, cars)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stuff classes"}),": Uncountable materials (sky, road)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"depth-estimation",children:"Depth Estimation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Stereo Vision"}),": Calculate depth from multiple camera views"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"depth = (baseline * focal_length) / disparity\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Monocular Depth Estimation"}),": Deep learning from single images\n",(0,t.jsx)(n.strong,{children:"LiDAR Integration"}),": Combining vision with active depth sensing"]}),"\n",(0,t.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,t.jsx)(n.h3,{id:"definition-and-applications",children:"Definition and Applications"}),"\n",(0,t.jsx)(n.p,{children:"Visual servoing uses visual feedback to control robot motion:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Move robot to achieve desired visual configuration\n",(0,t.jsx)(n.strong,{children:"Feedback"}),": Image features (points, lines, objects)\n",(0,t.jsx)(n.strong,{children:"Control"}),": Joint or Cartesian space commands"]}),"\n",(0,t.jsx)(n.h3,{id:"image-based-visual-servoing-ibvs",children:"Image-Based Visual Servoing (IBVS)"}),"\n",(0,t.jsx)(n.p,{children:"Control directly in image space:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u1e8a = J_x * v = J_x * J_v\u207b\xb9 * J_\u03b8 * \u03b8\u0307\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u1e8a = image velocity"}),"\n",(0,t.jsx)(n.li,{children:"J_x = image Jacobian"}),"\n",(0,t.jsx)(n.li,{children:"v = Cartesian velocity"}),"\n",(0,t.jsx)(n.li,{children:"J_v = velocity Jacobian"}),"\n",(0,t.jsx)(n.li,{children:"J_\u03b8 = kinematic Jacobian"}),"\n",(0,t.jsx)(n.li,{children:"\u03b8\u0307 = joint velocity"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advantages"}),": Direct image feature control\n",(0,t.jsx)(n.strong,{children:"Disadvantages"}),": May pass through singularities"]}),"\n",(0,t.jsx)(n.h3,{id:"position-based-visual-servoing-pbvs",children:"Position-Based Visual Servoing (PBVS)"}),"\n",(0,t.jsx)(n.p,{children:"Control in Cartesian space using 3D positions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"v = -\u03bb * J\u207b\xb9 * e\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"v = Cartesian velocity"}),"\n",(0,t.jsx)(n.li,{children:"J = interaction matrix"}),"\n",(0,t.jsx)(n.li,{children:"e = error vector"}),"\n",(0,t.jsx)(n.li,{children:"\u03bb = gain parameter"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advantages"}),": Predictable Cartesian motion\n",(0,t.jsx)(n.strong,{children:"Disadvantages"}),": Requires 3D structure estimation"]}),"\n",(0,t.jsx)(n.h3,{id:"hybrid-approaches",children:"Hybrid Approaches"}),"\n",(0,t.jsx)(n.p,{children:"Combine image and position-based control:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use image features for orientation"}),"\n",(0,t.jsx)(n.li,{children:"Use 3D positions for translation"}),"\n",(0,t.jsx)(n.li,{children:"Better robustness and convergence"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"slam-simultaneous-localization-and-mapping",children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,t.jsx)(n.h3,{id:"slam-fundamentals",children:"SLAM Fundamentals"}),"\n",(0,t.jsx)(n.p,{children:"SLAM estimates robot trajectory and map simultaneously:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"P(x_t, m | z_1:t, u_1:t, x_0)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"x_t = robot pose at time t"}),"\n",(0,t.jsx)(n.li,{children:"m = map"}),"\n",(0,t.jsx)(n.li,{children:"z_t = observations at time t"}),"\n",(0,t.jsx)(n.li,{children:"u_t = controls at time t"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"feature-based-slam",children:"Feature-based SLAM"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Front-end"}),": Extract and match features\n",(0,t.jsx)(n.strong,{children:"Back-end"}),": Optimize pose and map estimates\n",(0,t.jsx)(n.strong,{children:"Loop Closure"}),": Detect revisited locations"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ORB-SLAM"}),": Real-time SLAM with ORB features\n",(0,t.jsx)(n.strong,{children:"LSD-SLAM"}),": Direct method using line segments\n",(0,t.jsx)(n.strong,{children:"SVO"}),": Semi-direct visual odometry"]}),"\n",(0,t.jsx)(n.h3,{id:"direct-slam",children:"Direct SLAM"}),"\n",(0,t.jsx)(n.p,{children:"Use raw pixel intensities instead of features:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"DTAM"}),": Dense tracking and mapping\n",(0,t.jsx)(n.strong,{children:"LSD-SLAM"}),": Line segment detection and tracking\n",(0,t.jsx)(n.strong,{children:"SVO"}),": Semi-direct visual odometry"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advantages"}),": Works in textureless environments\n",(0,t.jsx)(n.strong,{children:"Disadvantages"}),": Sensitive to lighting changes"]}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-slam",children:"Deep Learning SLAM"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CNN-based Feature Extraction"}),": Learn optimal features\n",(0,t.jsx)(n.strong,{children:"End-to-End Learning"}),": Joint optimization of all components\n",(0,t.jsx)(n.strong,{children:"Uncertainty Estimation"}),": Learned uncertainty representations"]}),"\n",(0,t.jsx)(n.h2,{id:"multi-modal-perception",children:"Multi-modal Perception"}),"\n",(0,t.jsx)(n.h3,{id:"sensor-fusion-with-vision",children:"Sensor Fusion with Vision"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision + IMU"}),": Visual-inertial odometry (VIO)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual features for positioning"}),"\n",(0,t.jsx)(n.li,{children:"IMU for motion prediction"}),"\n",(0,t.jsx)(n.li,{children:"Robust to motion blur and fast motion"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision + LiDAR"}),": Dense mapping and localization"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"LiDAR for accurate depth"}),"\n",(0,t.jsx)(n.li,{children:"Vision for semantic information"}),"\n",(0,t.jsx)(n.li,{children:"Complementary strengths"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision + Tactile"}),": Active exploration and recognition"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vision for pre-grasp planning"}),"\n",(0,t.jsx)(n.li,{children:"Tactile for contact verification"}),"\n",(0,t.jsx)(n.li,{children:"Haptic feedback for manipulation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-learning",children:"Cross-modal Learning"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language Models"}),": Understanding natural language commands\n",(0,t.jsx)(n.strong,{children:"Vision-Audio Integration"}),": Sound localization and scene understanding\n",(0,t.jsx)(n.strong,{children:"Vision-Touch Fusion"}),": Improved object property estimation"]}),"\n",(0,t.jsx)(n.h2,{id:"technical-depth-mathematical-foundations",children:"Technical Depth: Mathematical Foundations"}),"\n",(0,t.jsx)(n.h3,{id:"camera-models",children:"Camera Models"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pinhole Camera Model"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[u]   [f_x  0   c_x] [R | t] [X]\n[v] = [0   f_y  c_y] [0 | 1] [Y]\n[1]   [0   0    1  ]         [Z]\n                          [1]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"(u,v) = pixel coordinates"}),"\n",(0,t.jsx)(n.li,{children:"(f_x,f_y) = focal lengths"}),"\n",(0,t.jsx)(n.li,{children:"(c_x,c_y) = principal point"}),"\n",(0,t.jsx)(n.li,{children:"[R|t] = extrinsic parameters"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Distortion Models"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"x_corrected = x * (1 + k_1*r\xb2 + k_2*r\u2074 + k_3*r\u2076) + 2*p_1*x*y + p_2*(r\xb2 + 2*x\xb2)\ny_corrected = y * (1 + k_1*r\xb2 + k_2*r\u2074 + k_3*r\u2076) + p_1*(r\xb2 + 2*y\xb2) + 2*p_2*x*y\n"})}),"\n",(0,t.jsx)(n.h3,{id:"epipolar-geometry",children:"Epipolar Geometry"}),"\n",(0,t.jsx)(n.p,{children:"For stereo vision and multi-view geometry:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fundamental Matrix"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"x\u2082\u1d40 * F * x\u2081 = 0\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Essential Matrix"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"x\u2082\u1d40 * E * x\u2081 = 0\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where E = K\u2082\u1d40 * F * K\u2081"}),"\n",(0,t.jsx)(n.h3,{id:"visual-jacobian",children:"Visual Jacobian"}),"\n",(0,t.jsx)(n.p,{children:"Relate image feature velocities to camera motion:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u1e57 = J_p * \u03be\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u1e57 = image feature velocity"}),"\n",(0,t.jsx)(n.li,{children:"J_p = interaction matrix"}),"\n",(0,t.jsx)(n.li,{children:"\u03be = camera velocity (6 DOF)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-applications",children:"Practical Applications"}),"\n",(0,t.jsx)(n.h3,{id:"navigation-and-mapping",children:"Navigation and Mapping"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visual Navigation"}),": Using vision for path planning and obstacle avoidance\n",(0,t.jsx)(n.strong,{children:"Semantic Mapping"}),": Creating maps with object and area labels\n",(0,t.jsx)(n.strong,{children:"Dynamic Scene Handling"}),": Dealing with moving objects and changing environments"]}),"\n",(0,t.jsx)(n.h3,{id:"manipulation-and-grasping",children:"Manipulation and Grasping"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visual Servoing for Grasping"}),": Precise positioning for object manipulation\n",(0,t.jsx)(n.strong,{children:"Object Pose Estimation"}),": 6D pose for robotic manipulation\n",(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Using visual information to plan grasps"]}),"\n",(0,t.jsx)(n.h3,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Gaze Following"}),": Directing attention based on human gaze\n",(0,t.jsx)(n.strong,{children:"Gesture Recognition"}),": Understanding human gestures and actions\n",(0,t.jsx)(n.strong,{children:"Social Navigation"}),": Navigating around humans safely"]}),"\n",(0,t.jsx)(n.h2,{id:"challenges",children:"Challenges"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,t.jsx)(n.p,{children:"Processing high-resolution images at control rates (100-1000Hz) is computationally demanding."}),"\n",(0,t.jsx)(n.h3,{id:"environmental-variability",children:"Environmental Variability"}),"\n",(0,t.jsx)(n.p,{children:"Lighting changes, occlusions, and dynamic environments affect recognition performance."}),"\n",(0,t.jsx)(n.h3,{id:"scale-and-resource-constraints",children:"Scale and Resource Constraints"}),"\n",(0,t.jsx)(n.p,{children:"Limited computational power and memory on robot platforms."}),"\n",(0,t.jsx)(n.h3,{id:"integration-complexity",children:"Integration Complexity"}),"\n",(0,t.jsx)(n.p,{children:"Fusing vision with other sensors and control systems requires careful coordination."}),"\n",(0,t.jsx)(n.h2,{id:"figure-list",children:"Figure List"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure 10.1"}),": Computer vision pipeline for robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure 10.2"}),": Object detection and recognition framework"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure 10.3"}),": Visual servoing control loop"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure 10.4"}),": SLAM system architecture"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure 10.5"}),": Multi-modal perception integration"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"code-example-computer-vision-implementation",children:"Code Example: Computer Vision Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport cv2\nfrom typing import List, Tuple, Optional, Dict\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom scipy.spatial.transform import Rotation as R\nimport time\n\n@dataclass\nclass ImageFeature:\n    \"\"\"Represents a visual feature in an image\"\"\"\n    point: np.ndarray  # 2D image coordinates [u, v]\n    descriptor: np.ndarray  # Feature descriptor vector\n    id: int  # Unique feature identifier\n    scale: float = 1.0  # Scale of detection\n\n@dataclass\nclass ObjectDetection:\n    \"\"\"Result of object detection\"\"\"\n    bbox: np.ndarray  # Bounding box [x1, y1, x2, y2]\n    confidence: float  # Detection confidence\n    class_id: int  # Object class identifier\n    class_name: str  # Object class name\n    mask: Optional[np.ndarray] = None  # Segmentation mask\n\n@dataclass\nclass CameraIntrinsics:\n    \"\"\"Camera intrinsic parameters\"\"\"\n    fx: float  # Focal length x\n    fy: float  # Focal length y\n    cx: float  # Principal point x\n    cy: float  # Principal point y\n    k1: float = 0.0  # Radial distortion k1\n    k2: float = 0.0  # Radial distortion k2\n    p1: float = 0.0  # Tangential distortion p1\n    p2: float = 0.0  # Tangential distortion p2\n\nclass FeatureExtractor:\n    \"\"\"Extract visual features from images\"\"\"\n\n    def __init__(self, method: str = \"orb\", max_features: int = 1000):\n        self.method = method\n        self.max_features = max_features\n\n        if method == \"orb\":\n            self.detector = cv2.ORB_create(nfeatures=max_features)\n        elif method == \"sift\":\n            self.detector = cv2.SIFT_create(nfeatures=max_features)\n        elif method == \"akaze\":\n            self.detector = cv2.AKAZE_create()\n\n    def extract_features(self, image: np.ndarray) -> List[ImageFeature]:\n        \"\"\"Extract features from an image\"\"\"\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Detect keypoints and compute descriptors\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n\n        if keypoints is None or descriptors is None:\n            return []\n\n        # Convert to our format\n        features = []\n        for i, (kp, desc) in enumerate(zip(keypoints, descriptors)):\n            feature = ImageFeature(\n                point=np.array([kp.pt[0], kp.pt[1]]),\n                descriptor=desc,\n                id=i,\n                scale=kp.size\n            )\n            features.append(feature)\n\n        return features[:self.max_features]\n\n    def match_features(self, features1: List[ImageFeature],\n                      features2: List[ImageFeature]) -> List[Tuple[int, int]]:\n        \"\"\"Match features between two sets\"\"\"\n        if not features1 or not features2:\n            return []\n\n        # Extract descriptors\n        desc1 = np.array([f.descriptor for f in features1])\n        desc2 = np.array([f.descriptor for f in features2])\n\n        # Use brute force matcher\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING if self.method == \"orb\" else cv2.NORM_L2)\n\n        matches = bf.knnMatch(desc1, desc2, k=2)\n\n        # Apply ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.75 * n.distance:\n                    good_matches.append((m.queryIdx, m.trainIdx))\n\n        return good_matches\n\nclass ObjectDetector:\n    \"\"\"Object detection for robotics applications\"\"\"\n\n    def __init__(self, model_path: Optional[str] = None):\n        self.model_path = model_path\n        self.model = self._load_model()\n        self.class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n            'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n            'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n            'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n            'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n            'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def _load_model(self):\n        \"\"\"Load object detection model (simplified)\"\"\"\n        # In a real implementation, this would load a trained model\n        # For this example, we'll create a simple mock detector\n        return \"mock_model\"\n\n    def detect_objects(self, image: np.ndarray, confidence_threshold: float = 0.5) -> List[ObjectDetection]:\n        \"\"\"Detect objects in an image\"\"\"\n        # In a real implementation, this would run the detection model\n        # For this example, we'll simulate detection results\n\n        # Convert image to suitable format if needed\n        if len(image.shape) == 3:\n            height, width, _ = image.shape\n        else:\n            height, width = image.shape\n\n        # Simulate some detections\n        detections = []\n\n        # Add a few mock detections\n        mock_objects = [\n            (width * 0.2, height * 0.3, width * 0.4, height * 0.6, 0.8, 0, \"person\"),\n            (width * 0.6, height * 0.4, width * 0.8, height * 0.7, 0.9, 56, \"chair\"),\n            (width * 0.4, height * 0.5, width * 0.6, height * 0.8, 0.7, 39, \"bottle\")\n        ]\n\n        for x1, y1, x2, y2, conf, class_id, class_name in mock_objects:\n            if conf >= confidence_threshold:\n                detection = ObjectDetection(\n                    bbox=np.array([x1, y1, x2, y2]),\n                    confidence=conf,\n                    class_id=class_id,\n                    class_name=class_name\n                )\n                detections.append(detection)\n\n        return detections\n\n    def detect_and_track(self, image: np.ndarray, prev_detections: List[ObjectDetection] = None) -> List[ObjectDetection]:\n        \"\"\"Detect and potentially track objects across frames\"\"\"\n        current_detections = self.detect_objects(image)\n\n        # In a real implementation, this would include tracking logic\n        # For this example, we'll just return the current detections\n        return current_detections\n\nclass VisualServoingController:\n    \"\"\"Controller for visual servoing applications\"\"\"\n\n    def __init__(self, camera_intrinsics: CameraIntrinsics,\n                 control_gain: float = 1.0):\n        self.camera_intrinsics = camera_intrinsics\n        self.control_gain = control_gain\n        self.reference_features = None\n        self.current_features = None\n\n    def set_reference_image(self, image: np.ndarray, features: List[ImageFeature]):\n        \"\"\"Set the reference image and features for visual servoing\"\"\"\n        self.reference_features = features\n        self.reference_image = image\n\n    def compute_image_jacobian(self, point: np.ndarray) -> np.ndarray:\n        \"\"\"Compute image Jacobian for a 2D point\"\"\"\n        # Simplified image Jacobian computation\n        # This is a basic version - real implementation would be more complex\n        u, v = point\n\n        # Camera intrinsic parameters\n        fx, fy = self.camera_intrinsics.fx, self.camera_intrinsics.fy\n        cx, cy = self.camera_intrinsics.cx, self.camera_intrinsics.cy\n\n        # Image Jacobian (interaction matrix)\n        L = np.zeros((2, 6))\n        L[0, 0] = -fx  # \u2202u/\u2202x\n        L[0, 2] = -(u - cx)  # \u2202u/\u2202z\n        L[0, 3] = -(u - cx) * (v - cy) / fy  # \u2202u/\u2202\u03c9x\n        L[0, 4] = (fx + (u - cx) * (u - cx) / fx)  # \u2202u/\u2202\u03c9y\n        L[0, 5] = -(v - cy)  # \u2202u/\u2202\u03c9z\n\n        L[1, 1] = -fy  # \u2202v/\u2202y\n        L[1, 2] = -(v - cy)  # \u2202v/\u2202z\n        L[1, 3] = -(fy + (v - cy) * (v - cy) / fy)  # \u2202v/\u2202\u03c9x\n        L[1, 4] = (u - cx) * (v - cy) / fx  # \u2202v/\u2202\u03c9y\n        L[1, 5] = (u - cx)  # \u2202v/\u2202\u03c9z\n\n        return L\n\n    def compute_control(self, current_features: List[ImageFeature]) -> np.ndarray:\n        \"\"\"Compute control velocities based on feature errors\"\"\"\n        if not self.reference_features or not current_features:\n            return np.zeros(6)  # No motion if no features\n\n        # Match features between reference and current\n        extractor = FeatureExtractor()\n        matches = extractor.match_features(self.reference_features, current_features)\n\n        if not matches:\n            return np.zeros(6)\n\n        # Compute error for matched features\n        errors = []\n        jacobians = []\n\n        for ref_idx, curr_idx in matches[:10]:  # Limit to first 10 matches\n            if ref_idx < len(self.reference_features) and curr_idx < len(current_features):\n                ref_pt = self.reference_features[ref_idx].point\n                curr_pt = current_features[curr_idx].point\n\n                error = curr_pt - ref_pt\n                jacobian = self.compute_image_jacobian(ref_pt)\n\n                errors.append(error)\n                jacobians.append(jacobian)\n\n        if not errors:\n            return np.zeros(6)\n\n        # Stack errors and jacobians\n        E = np.hstack(errors)  # Error vector\n        J = np.vstack(jacobians)  # Stacked Jacobians\n\n        # Compute control using pseudo-inverse\n        try:\n            # Regularized control law\n            lambda_reg = 0.01\n            control_vel = -self.control_gain * np.linalg.pinv(J.T @ J + lambda_reg * np.eye(6)) @ J.T @ E\n        except np.linalg.LinAlgError:\n            # Fallback to damped least squares if matrix is singular\n            control_vel = np.zeros(6)\n\n        # Limit control velocities\n        max_vel = 0.1  # 10 cm/s\n        control_vel = np.clip(control_vel, -max_vel, max_vel)\n\n        return control_vel\n\nclass SLAMSystem:\n    \"\"\"Basic SLAM system for visual mapping and localization\"\"\"\n\n    def __init__(self, camera_intrinsics: CameraIntrinsics):\n        self.camera_intrinsics = camera_intrinsics\n        self.map_points = {}  # 3D points in global map\n        self.keyframes = []  # Camera poses\n        self.current_pose = np.eye(4)  # Current camera pose\n        self.feature_extractor = FeatureExtractor()\n        self.min_triangulation_angle = 10  # degrees\n\n    def triangulate_point(self,\n                         pose1: np.ndarray,\n                         pose2: np.ndarray,\n                         point1: np.ndarray,\n                         point2: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Triangulate 3D point from two camera views\"\"\"\n        # Camera intrinsic matrix\n        K = np.array([\n            [self.camera_intrinsics.fx, 0, self.camera_intrinsics.cx],\n            [0, self.camera_intrinsics.fy, self.camera_intrinsics.cy],\n            [0, 0, 1]\n        ])\n\n        # Convert poses to projection matrices\n        P1 = K @ pose1[:3, :]  # First camera\n        P2 = K @ pose2[:3, :]  # Second camera\n\n        # Linear triangulation (using SVD)\n        A = np.zeros((4, 4))\n        A[0, :] = point1[0] * P1[2, :] - P1[0, :]\n        A[1, :] = point1[1] * P1[2, :] - P1[1, :]\n        A[2, :] = point2[0] * P2[2, :] - P2[0, :]\n        A[3, :] = point2[1] * P2[2, :] - P2[1, :]\n\n        # Solve using SVD\n        _, _, V = np.linalg.svd(A)\n        X = V[-1, :]\n        X = X / X[3]  # Normalize\n\n        if X[3] != 0:\n            X = X[:3] / X[3]\n            return X\n        else:\n            return None\n\n    def add_keyframe(self, image: np.ndarray, pose: np.ndarray):\n        \"\"\"Add a new keyframe to the map\"\"\"\n        features = self.feature_extractor.extract_features(image)\n\n        keyframe = {\n            'image': image,\n            'pose': pose,\n            'features': features,\n            'timestamp': time.time()\n        }\n\n        self.keyframes.append(keyframe)\n\n    def estimate_motion(self,\n                       image1: np.ndarray,\n                       image2: np.ndarray) -> Tuple[np.ndarray, float]:\n        \"\"\"Estimate relative motion between two images\"\"\"\n        # Extract features from both images\n        features1 = self.feature_extractor.extract_features(image1)\n        features2 = self.feature_extractor.extract_features(image2)\n\n        # Match features\n        matches = self.feature_extractor.match_features(features1, features2)\n\n        if len(matches) < 8:  # Minimum for pose estimation\n            return np.eye(4), 0.0  # No motion\n\n        # Get matched points\n        pts1 = np.float32([features1[m[0]].point for m in matches]).reshape(-1, 1, 2)\n        pts2 = np.float32([features2[m[1]].point for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate essential matrix\n        E, mask = cv2.findEssentialMat(pts1, pts2,\n                                      np.array([[self.camera_intrinsics.fx, 0, self.camera_intrinsics.cx],\n                                               [0, self.camera_intrinsics.fy, self.camera_intrinsics.cy],\n                                               [0, 0, 1]]),\n                                      method=cv2.RANSAC, threshold=1.0)\n\n        if E is None:\n            return np.eye(4), 0.0\n\n        # Recover pose from essential matrix\n        _, R, t, _ = cv2.recoverPose(E, pts1, pts2)\n\n        # Create transformation matrix\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = t.ravel()\n\n        # Calculate motion magnitude\n        motion_magnitude = np.linalg.norm(t) + np.arccos(np.clip((np.trace(R) - 1) / 2, -1, 1))\n\n        return T, motion_magnitude\n\n    def build_map(self, images: List[np.ndarray]) -> Dict:\n        \"\"\"Build a simple map from a sequence of images\"\"\"\n        if len(images) < 2:\n            return {'points': [], 'poses': []}\n\n        # Add first keyframe\n        self.add_keyframe(images[0], np.eye(4))\n\n        # Process remaining images\n        current_pose = np.eye(4)\n        all_poses = [current_pose.copy()]\n\n        for i in range(1, len(images)):\n            # Estimate motion\n            motion, motion_mag = self.estimate_motion(images[i-1], images[i])\n\n            if motion_mag > 0.01:  # Only add if significant motion\n                current_pose = current_pose @ motion\n                self.add_keyframe(images[i], current_pose)\n                all_poses.append(current_pose.copy())\n\n        # Return map information\n        return {\n            'num_keyframes': len(self.keyframes),\n            'total_poses': len(all_poses),\n            'final_pose': current_pose,\n            'estimated_trajectory_length': len(all_poses)\n        }\n\ndef demonstrate_vision_systems():\n    \"\"\"Demonstrate computer vision concepts\"\"\"\n    print(\"Computer Vision and Perception - Chapter 10\")\n    print(\"=\" * 50)\n\n    # Initialize camera intrinsics (typical for robot head camera)\n    camera_intrinsics = CameraIntrinsics(\n        fx=525.0,  # Focal length in pixels\n        fy=525.0,\n        cx=319.5,  # Principal point\n        cy=239.5\n    )\n\n    print(\"1. Feature Extraction Demo:\")\n    # Create a sample image (in practice, this would come from a camera)\n    sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n    # Add some artificial features (simulated)\n    cv2.rectangle(sample_image, (100, 100), (200, 200), (255, 0, 0), 2)\n    cv2.circle(sample_image, (300, 300), 20, (0, 255, 0), 2)\n\n    # Extract features using different methods\n    orb_extractor = FeatureExtractor(method=\"orb\", max_features=50)\n    orb_features = orb_extractor.extract_features(sample_image)\n    print(f\"   - ORB features extracted: {len(orb_features)}\")\n\n    # Object detection demo\n    print(\"\\n2. Object Detection Demo:\")\n    detector = ObjectDetector()\n    detections = detector.detect_objects(sample_image)\n    print(f\"   - Objects detected: {len(detections)}\")\n    for det in detections[:3]:  # Show first 3 detections\n        bbox_str = f\"[{det.bbox[0]:.1f}, {det.bbox[1]:.1f}, {det.bbox[2]:.1f}, {det.bbox[3]:.1f}]\"\n        print(f\"     - {det.class_name}: {det.confidence:.2f}, bbox: {bbox_str}\")\n\n    # Visual servoing demo\n    print(\"\\n3. Visual Servoing Demo:\")\n    servo_controller = VisualServoingController(camera_intrinsics)\n\n    # Simulate reference and current features\n    ref_features = [\n        ImageFeature(point=np.array([100, 100]), descriptor=np.random.rand(32), id=0),\n        ImageFeature(point=np.array([200, 150]), descriptor=np.random.rand(32), id=1),\n        ImageFeature(point=np.array([150, 200]), descriptor=np.random.rand(32), id=2)\n    ]\n\n    curr_features = [\n        ImageFeature(point=np.array([105, 105]), descriptor=np.random.rand(32), id=0),  # Slightly moved\n        ImageFeature(point=np.array([205, 155]), descriptor=np.random.rand(32), id=1),  # Slightly moved\n        ImageFeature(point=np.array([155, 205]), descriptor=np.random.rand(32), id=2)   # Slightly moved\n    ]\n\n    servo_controller.set_reference_image(sample_image, ref_features)\n    control_vel = servo_controller.compute_control(curr_features)\n    print(f\"   - Computed control velocity: [{control_vel[0]:.3f}, {control_vel[1]:.3f}, {control_vel[2]:.3f}, \"\n          f\"{control_vel[3]:.3f}, {control_vel[4]:.3f}, {control_vel[5]:.3f}]\")\n\n    # SLAM demo\n    print(\"\\n4. SLAM Demo:\")\n    slam_system = SLAMSystem(camera_intrinsics)\n\n    # Simulate a sequence of images (in practice, these would be captured from camera)\n    simulated_images = []\n    for i in range(5):\n        img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        # Add distinctive features\n        cv2.rectangle(img, (50 + i*20, 50 + i*10), (100 + i*20, 100 + i*10), (255, 255, 0), 2)\n        simulated_images.append(img)\n\n    slam_result = slam_system.build_map(simulated_images)\n    print(f\"   - SLAM results:\")\n    print(f\"     - Keyframes created: {slam_result['num_keyframes']}\")\n    print(f\"     - Total poses: {slam_result['total_poses']}\")\n    print(f\"     - Final position: [{slam_result['final_pose'][0,3]:.3f}, \"\n          f\"{slam_result['final_pose'][1,3]:.3f}, {slam_result['final_pose'][2,3]:.3f}]\")\n\n    # Performance analysis\n    print(\"\\n5. Performance Analysis:\")\n\n    # Feature extraction performance\n    start_time = time.time()\n    for _ in range(10):\n        orb_extractor.extract_features(sample_image)\n    end_time = time.time()\n    avg_feature_time = (end_time - start_time) / 10\n    print(f\"   - Average feature extraction time: {avg_feature_time:.4f}s ({1/avg_feature_time:.1f} Hz)\")\n\n    # Object detection performance\n    start_time = time.time()\n    for _ in range(5):\n        detector.detect_objects(sample_image)\n    end_time = time.time()\n    avg_detection_time = (end_time - start_time) / 5\n    print(f\"   - Average detection time: {avg_detection_time:.4f}s ({1/avg_detection_time:.1f} Hz)\")\n\n    # Memory usage estimation\n    import sys\n    image_memory = sys.getsizeof(sample_image.tobytes())\n    print(f\"   - Sample image memory: {image_memory / 1024:.1f} KB\")\n\n    print(\"\\n6. Multi-modal Integration:\")\n    print(\"   - Vision can be integrated with:\")\n    print(\"     - IMU for visual-inertial odometry\")\n    print(\"     - LiDAR for dense mapping\")\n    print(\"     - Tactile sensors for active exploration\")\n    print(\"     - Audio for scene understanding\")\n\n    return {\n        'features': len(orb_features),\n        'detections': len(detections),\n        'control_velocity': control_vel,\n        'slam_results': slam_result\n    }\n\ndef analyze_vision_performance(results: Dict) -> Dict:\n    \"\"\"Analyze computer vision performance metrics\"\"\"\n    analysis = {\n        'detection_performance': {\n            'objects_detected': results['detections'],\n            'detection_rate': 'Simulated'  # Would be calculated from real data\n        },\n        'feature_performance': {\n            'features_extracted': results['features'],\n            'feature_density': results['features'] / (640 * 480) * 10000  # per 10,000 pixels\n        },\n        'control_performance': {\n            'max_control_magnitude': np.max(np.abs(results['control_velocity'])),\n            'control_dimensions': len(results['control_velocity'])\n        },\n        'mapping_performance': {\n            'keyframes_created': results['slam_results']['num_keyframes'],\n            'trajectory_length': results['slam_results']['estimated_trajectory_length']\n        }\n    }\n\n    return analysis\n\nif __name__ == \"__main__\":\n    # Run the demonstration\n    results = demonstrate_vision_systems()\n\n    # Analyze performance\n    performance_analysis = analyze_vision_performance(results)\n\n    print(f\"\\n7. Performance Analysis Summary:\")\n    for category, metrics in performance_analysis.items():\n        print(f\"\\n   {category.replace('_', ' ').title()}:\")\n        for metric, value in metrics.items():\n            print(f\"     - {metric.replace('_', ' ')}: {value}\")\n\n    print(f\"\\n8. Vision System Capabilities:\")\n    print(\"   - Real-time object detection and recognition\")\n    print(\"   - Visual servoing for precise control\")\n    print(\"   - SLAM for mapping and localization\")\n    print(\"   - Multi-modal sensor integration\")\n    print(\"   - Robust feature extraction and matching\")\n\n    print(f\"\\nComputer Vision and Perception - Chapter 10 Complete!\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement a simple SLAM system using ORB features and essential matrix decomposition."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Design a visual servoing controller that can guide a robot arm to grasp an object."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create an object detection pipeline that works with depth information for 3D object localization."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary-1",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter provided a comprehensive overview of computer vision and perception systems for humanoid robots, covering object detection, scene understanding, visual servoing, SLAM systems, and multi-modal integration. We explored mathematical foundations, practical implementations, and the integration of vision with other sensory modalities. The concepts and code examples presented will help in developing robust vision systems for humanoid robots that can perceive and interact with their environment effectively."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);