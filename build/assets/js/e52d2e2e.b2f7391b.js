"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[695],{775:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=t(4848),s=t(8453);const a={sidebar_position:3},o="Chapter 12: Decision Making and Autonomy",r={id:"modules/module-4-ai/chapter-12-decision-making",title:"Chapter 12: Decision Making and Autonomy",description:"Summary",source:"@site/docs/modules/module-4-ai/chapter-12-decision-making.md",sourceDirName:"modules/module-4-ai",slug:"/modules/module-4-ai/chapter-12-decision-making",permalink:"/docs/modules/module-4-ai/chapter-12-decision-making",draft:!1,unlisted:!1,editUrl:"https://github.com/Mehwish-Malik/AI-Robotics-Book.git/docs/modules/module-4-ai/chapter-12-decision-making.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Chapter 11: Machine Learning for Robotics",permalink:"/docs/modules/module-4-ai/chapter-11-learning"},next:{title:"Chapter 13: Simulation Environments",permalink:"/docs/modules/module-5-applications/chapter-13-simulation"}},l={},c=[{value:"Summary",id:"summary",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Introduction to Autonomous Decision Making",id:"introduction-to-autonomous-decision-making",level:2},{value:"Autonomy Requirements",id:"autonomy-requirements",level:3},{value:"Decision Making Architecture",id:"decision-making-architecture",level:3},{value:"Task Planning and Scheduling",id:"task-planning-and-scheduling",level:2},{value:"Hierarchical Task Networks (HTN)",id:"hierarchical-task-networks-htn",level:3},{value:"Partial Order Planning",id:"partial-order-planning",level:3},{value:"Temporal Planning",id:"temporal-planning",level:3},{value:"Contingent Planning",id:"contingent-planning",level:3},{value:"Reasoning Under Uncertainty",id:"reasoning-under-uncertainty",level:2},{value:"Probabilistic Reasoning",id:"probabilistic-reasoning",level:3},{value:"Dempster-Shafer Theory",id:"dempster-shafer-theory",level:3},{value:"Fuzzy Logic",id:"fuzzy-logic",level:3},{value:"Decision Theory",id:"decision-theory",level:3},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Gesture Recognition",id:"gesture-recognition",level:3},{value:"Social Cues",id:"social-cues",level:3},{value:"Multi-modal Interaction",id:"multi-modal-interaction",level:3},{value:"Multi-modal Interaction Systems",id:"multi-modal-interaction-systems",level:2},{value:"Sensor Fusion for Interaction",id:"sensor-fusion-for-interaction",level:3},{value:"Context Awareness",id:"context-awareness",level:3},{value:"Adaptive Interaction",id:"adaptive-interaction",level:3},{value:"Ethical Considerations",id:"ethical-considerations",level:2},{value:"Safety and Risk Assessment",id:"safety-and-risk-assessment",level:3},{value:"Privacy and Data Protection",id:"privacy-and-data-protection",level:3},{value:"Transparency and Explainability",id:"transparency-and-explainability",level:3},{value:"Bias and Fairness",id:"bias-and-fairness",level:3},{value:"Technical Depth: Mathematical Foundations",id:"technical-depth-mathematical-foundations",level:2},{value:"Markov Decision Processes (MDPs)",id:"markov-decision-processes-mdps",level:3},{value:"Partially Observable MDPs (POMDPs)",id:"partially-observable-mdps-pomdps",level:3},{value:"Planning as Inference",id:"planning-as-inference",level:3},{value:"Game Theory Applications",id:"game-theory-applications",level:3},{value:"Practical Applications",id:"practical-applications",level:2},{value:"Service Robotics",id:"service-robotics",level:3},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research and Development",id:"research-and-development",level:3},{value:"Challenges",id:"challenges",level:2},{value:"Computational Complexity",id:"computational-complexity",level:3},{value:"Uncertainty Management",id:"uncertainty-management",level:3},{value:"Human Acceptance",id:"human-acceptance",level:3},{value:"Safety Assurance",id:"safety-assurance",level:3},{value:"Figure List",id:"figure-list",level:2},{value:"Code Example: Decision Making and Autonomy Implementation",id:"code-example-decision-making-and-autonomy-implementation",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary-1",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-12-decision-making-and-autonomy",children:"Chapter 12: Decision Making and Autonomy"}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter explores the artificial intelligence systems that enable humanoid robots to make autonomous decisions and operate independently. We'll examine task planning, reasoning under uncertainty, human-robot interaction, and the ethical considerations of autonomous robotic systems. Understanding decision-making systems is crucial for creating robots that can operate effectively in complex, dynamic environments without constant human supervision."}),"\n",(0,i.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Design task planning systems for complex robotic behaviors"}),"\n",(0,i.jsx)(e.li,{children:"Implement reasoning systems that handle uncertainty"}),"\n",(0,i.jsx)(e.li,{children:"Create effective human-robot interaction interfaces"}),"\n",(0,i.jsx)(e.li,{children:"Address ethical considerations in autonomous robotics"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate autonomy levels and decision-making capabilities"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Planning"}),": Decomposing high-level goals into executable actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reasoning Under Uncertainty"}),": Making decisions with incomplete information"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Effective communication and collaboration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Autonomy Levels"}),": Degrees of independent operation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-modal Interaction"}),": Combining different interaction modalities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Ethical AI"}),": Responsible deployment of autonomous systems"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Explainable AI"}),": Understanding robot decision-making processes"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-autonomous-decision-making",children:"Introduction to Autonomous Decision Making"}),"\n",(0,i.jsx)(e.p,{children:"Autonomous decision making in humanoid robots involves complex systems that must interpret high-level goals, assess environmental conditions, plan appropriate actions, and execute them while adapting to changes. Unlike pre-programmed robots, autonomous systems must handle uncertainty, learn from experience, and make real-time decisions."}),"\n",(0,i.jsx)(e.h3,{id:"autonomy-requirements",children:"Autonomy Requirements"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Perception"}),": Understanding the environment and robot state\n",(0,i.jsx)(e.strong,{children:"Reasoning"}),": Drawing conclusions from available information\n",(0,i.jsx)(e.strong,{children:"Planning"}),": Creating sequences of actions to achieve goals\n",(0,i.jsx)(e.strong,{children:"Execution"}),": Carrying out planned actions\n",(0,i.jsx)(e.strong,{children:"Monitoring"}),": Assessing progress and adapting as needed"]}),"\n",(0,i.jsx)(e.h3,{id:"decision-making-architecture",children:"Decision Making Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Goal Specification \u2192 Perception \u2192 Reasoning \u2192 Planning \u2192 Execution \u2192 Monitoring\n"})}),"\n",(0,i.jsx)(e.p,{children:"Each component must work seamlessly to enable effective autonomous operation."}),"\n",(0,i.jsx)(e.h2,{id:"task-planning-and-scheduling",children:"Task Planning and Scheduling"}),"\n",(0,i.jsx)(e.h3,{id:"hierarchical-task-networks-htn",children:"Hierarchical Task Networks (HTN)"}),"\n",(0,i.jsx)(e.p,{children:"Decompose complex tasks into simpler subtasks:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Goal: Serve drink\n\u251c\u2500\u2500 Navigate to kitchen\n\u2502   \u251c\u2500\u2500 Path planning\n\u2502   \u2514\u2500\u2500 Obstacle avoidance\n\u251c\u2500\u2500 Identify beverage\n\u2502   \u251c\u2500\u2500 Object recognition\n\u2502   \u2514\u2500\u2500 Classification\n\u251c\u2500\u2500 Grasp beverage\n\u2502   \u251c\u2500\u2500 Grasp planning\n\u2502   \u2514\u2500\u2500 Manipulation\n\u2514\u2500\u2500 Deliver to person\n    \u251c\u2500\u2500 Person detection\n    \u2514\u2500\u2500 Safe handover\n"})}),"\n",(0,i.jsx)(e.h3,{id:"partial-order-planning",children:"Partial Order Planning"}),"\n",(0,i.jsx)(e.p,{children:"Allow flexibility in action ordering while maintaining constraints:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Action A \u2500\u2500\u252c\u2500\u2500\u2192 Action C\n           \u2514\u2500\u2500\u2192 Action D \u2190\u2500\u2500 Action B\n"})}),"\n",(0,i.jsx)(e.h3,{id:"temporal-planning",children:"Temporal Planning"}),"\n",(0,i.jsx)(e.p,{children:"Consider timing constraints and concurrent actions:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Time: 0s\u2500\u2500\u2500\u2500\u25005s\u2500\u2500\u2500\u2500\u250010s\u2500\u2500\u2500\u2500\u250015s\nA:    [action1........]\nB:          [action2...]\nC:    [act3][act4]\n"})}),"\n",(0,i.jsx)(e.h3,{id:"contingent-planning",children:"Contingent Planning"}),"\n",(0,i.jsx)(e.p,{children:"Handle uncertain outcomes and alternative plans:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Plan: Go to room\n\u251c\u2500\u2500 If door is open \u2192 Walk through\n\u251c\u2500\u2500 If door is closed \u2192 Open door \u2192 Walk through\n\u2514\u2500\u2500 If door locked \u2192 Find alternative route\n"})}),"\n",(0,i.jsx)(e.h2,{id:"reasoning-under-uncertainty",children:"Reasoning Under Uncertainty"}),"\n",(0,i.jsx)(e.h3,{id:"probabilistic-reasoning",children:"Probabilistic Reasoning"}),"\n",(0,i.jsx)(e.p,{children:"Represent and reason with uncertain information:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Bayesian Networks"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"P(A|B) = P(B|A) * P(A) / P(B)\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Markov Models"}),": Sequences of states with transition probabilities\n",(0,i.jsx)(e.strong,{children:"Hidden Markov Models"}),": Observable outputs with hidden states"]}),"\n",(0,i.jsx)(e.h3,{id:"dempster-shafer-theory",children:"Dempster-Shafer Theory"}),"\n",(0,i.jsx)(e.p,{children:"Handle incomplete and conflicting evidence:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Bel(A) \u2264 Pl(A)\n"})}),"\n",(0,i.jsx)(e.p,{children:"Where Bel is belief and Pl is plausibility."}),"\n",(0,i.jsx)(e.h3,{id:"fuzzy-logic",children:"Fuzzy Logic"}),"\n",(0,i.jsx)(e.p,{children:"Handle imprecise concepts:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u03bc_tall(height) = {\n    0 if height < 160cm,\n    (height - 160)/40 if 160 \u2264 height \u2264 200,\n    1 if height > 200cm\n}\n"})}),"\n",(0,i.jsx)(e.h3,{id:"decision-theory",children:"Decision Theory"}),"\n",(0,i.jsx)(e.p,{children:"Make optimal decisions under uncertainty:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Expected Utility = \u03a3 P(outcome_i) * Utility(outcome_i)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,i.jsx)(e.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Intent Recognition"}),": Identify user's goal\n",(0,i.jsx)(e.strong,{children:"Entity Extraction"}),": Identify relevant objects/locations\n",(0,i.jsx)(e.strong,{children:"Dialogue Management"}),": Maintain conversation context"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'User: "Can you bring me the red cup?"\n\u251c\u2500\u2500 Intent: Deliver object\n\u251c\u2500\u2500 Entity: red cup (object)\n\u2514\u2500\u2500 Entity: me (recipient)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"gesture-recognition",children:"Gesture Recognition"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Static Gestures"}),": Hand poses and positions\n",(0,i.jsx)(e.strong,{children:"Dynamic Gestures"}),": Hand movements and trajectories\n",(0,i.jsx)(e.strong,{children:"Body Language"}),": Posture and movement interpretation"]}),"\n",(0,i.jsx)(e.h3,{id:"social-cues",children:"Social Cues"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Gaze Direction"}),": Understanding attention and focus\n",(0,i.jsx)(e.strong,{children:"Proxemics"}),": Respecting personal space\n",(0,i.jsx)(e.strong,{children:"Turn-taking"}),": Natural conversation flow\n",(0,i.jsx)(e.strong,{children:"Emotional Recognition"}),": Understanding human emotions"]}),"\n",(0,i.jsx)(e.h3,{id:"multi-modal-interaction",children:"Multi-modal Interaction"}),"\n",(0,i.jsx)(e.p,{children:"Combine multiple interaction modalities:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Speech + Gesture"}),": Natural human communication\n",(0,i.jsx)(e.strong,{children:"Speech + Vision"}),": Context-aware responses\n",(0,i.jsx)(e.strong,{children:"Touch + Vision"}),": Collaborative manipulation"]}),"\n",(0,i.jsx)(e.h2,{id:"multi-modal-interaction-systems",children:"Multi-modal Interaction Systems"}),"\n",(0,i.jsx)(e.h3,{id:"sensor-fusion-for-interaction",children:"Sensor Fusion for Interaction"}),"\n",(0,i.jsx)(e.p,{children:"Combine data from multiple sensors:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Audio Input \u2192 Speech Recognition\nVideo Input \u2192 Gesture Recognition\nHaptic Input \u2192 Touch Detection\n\u2192 Multi-modal Understanding\n"})}),"\n",(0,i.jsx)(e.h3,{id:"context-awareness",children:"Context Awareness"}),"\n",(0,i.jsx)(e.p,{children:"Maintain understanding of interaction context:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Spatial Context"}),": Where interactions occur\n",(0,i.jsx)(e.strong,{children:"Temporal Context"}),": When events happen\n",(0,i.jsx)(e.strong,{children:"Social Context"}),": Who is involved\n",(0,i.jsx)(e.strong,{children:"Activity Context"}),": What is happening"]}),"\n",(0,i.jsx)(e.h3,{id:"adaptive-interaction",children:"Adaptive Interaction"}),"\n",(0,i.jsx)(e.p,{children:"Adjust interaction style based on user:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"User Profiling"}),": Learn user preferences\n",(0,i.jsx)(e.strong,{children:"Adaptation Mechanisms"}),": Modify interaction parameters\n",(0,i.jsx)(e.strong,{children:"Learning from Feedback"}),": Improve interaction over time"]}),"\n",(0,i.jsx)(e.h2,{id:"ethical-considerations",children:"Ethical Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"safety-and-risk-assessment",children:"Safety and Risk Assessment"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Harm Prevention"}),": Avoid causing physical or psychological harm\n",(0,i.jsx)(e.strong,{children:"Risk Mitigation"}),": Identify and minimize potential risks\n",(0,i.jsx)(e.strong,{children:"Emergency Protocols"}),": Safe failure modes"]}),"\n",(0,i.jsx)(e.h3,{id:"privacy-and-data-protection",children:"Privacy and Data Protection"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Data Minimization"}),": Collect only necessary data\n",(0,i.jsx)(e.strong,{children:"Consent"}),": Obtain explicit permission for data use\n",(0,i.jsx)(e.strong,{children:"Security"}),": Protect collected information"]}),"\n",(0,i.jsx)(e.h3,{id:"transparency-and-explainability",children:"Transparency and Explainability"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Explainable AI"}),": Understanding robot decisions\n",(0,i.jsx)(e.strong,{children:"Decision Transparency"}),": Clear reasoning processes\n",(0,i.jsx)(e.strong,{children:"User Control"}),": Ability to override decisions"]}),"\n",(0,i.jsx)(e.h3,{id:"bias-and-fairness",children:"Bias and Fairness"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Algorithmic Bias"}),": Avoid discriminatory behavior\n",(0,i.jsx)(e.strong,{children:"Fair Treatment"}),": Equal interaction regardless of user characteristics\n",(0,i.jsx)(e.strong,{children:"Cultural Sensitivity"}),": Respect diverse cultural norms"]}),"\n",(0,i.jsx)(e.h2,{id:"technical-depth-mathematical-foundations",children:"Technical Depth: Mathematical Foundations"}),"\n",(0,i.jsx)(e.h3,{id:"markov-decision-processes-mdps",children:"Markov Decision Processes (MDPs)"}),"\n",(0,i.jsx)(e.p,{children:"Formal framework for sequential decision making:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"MDP = <S, A, T, R, \u03b3>\n"})}),"\n",(0,i.jsx)(e.p,{children:"Where:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"S = state space"}),"\n",(0,i.jsx)(e.li,{children:"A = action space"}),"\n",(0,i.jsx)(e.li,{children:"T = transition function T(s, a, s') = P(s'|s, a)"}),"\n",(0,i.jsx)(e.li,{children:"R = reward function R(s, a)"}),"\n",(0,i.jsx)(e.li,{children:"\u03b3 = discount factor"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"partially-observable-mdps-pomdps",children:"Partially Observable MDPs (POMDPs)"}),"\n",(0,i.jsx)(e.p,{children:"Handle partial observability:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"POMDP = <S, A, T, R, \u03a9, O, \u03b3>\n"})}),"\n",(0,i.jsx)(e.p,{children:"Where:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u03a9 = observation space"}),"\n",(0,i.jsx)(e.li,{children:"O = observation function O(s, o) = P(o|s)"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"planning-as-inference",children:"Planning as Inference"}),"\n",(0,i.jsx)(e.p,{children:"Formulate planning as probabilistic inference:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"P(action_sequence|goal, state) \u221d P(goal|action_sequence, state) * P(action_sequence)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"game-theory-applications",children:"Game Theory Applications"}),"\n",(0,i.jsx)(e.p,{children:"Model multi-agent interactions:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Nash Equilibrium"}),": Stable strategy profiles\n",(0,i.jsx)(e.strong,{children:"Stackelberg Games"}),": Leader-follower interactions\n",(0,i.jsx)(e.strong,{children:"Cooperative Games"}),": Collaborative decision making"]}),"\n",(0,i.jsx)(e.h2,{id:"practical-applications",children:"Practical Applications"}),"\n",(0,i.jsx)(e.h3,{id:"service-robotics",children:"Service Robotics"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Assistive Care"}),": Helping elderly and disabled individuals\n",(0,i.jsx)(e.strong,{children:"Hospitality"}),": Customer service and concierge functions\n",(0,i.jsx)(e.strong,{children:"Retail"}),": Customer assistance and inventory management"]}),"\n",(0,i.jsx)(e.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Collaborative Manufacturing"}),": Working alongside humans\n",(0,i.jsx)(e.strong,{children:"Quality Inspection"}),": Automated quality control\n",(0,i.jsx)(e.strong,{children:"Maintenance"}),": Predictive and preventive maintenance"]}),"\n",(0,i.jsx)(e.h3,{id:"research-and-development",children:"Research and Development"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Scientific Assistance"}),": Laboratory automation\n",(0,i.jsx)(e.strong,{children:"Data Collection"}),": Environmental monitoring\n",(0,i.jsx)(e.strong,{children:"Experimentation"}),": Autonomous research tasks"]}),"\n",(0,i.jsx)(e.h2,{id:"challenges",children:"Challenges"}),"\n",(0,i.jsx)(e.h3,{id:"computational-complexity",children:"Computational Complexity"}),"\n",(0,i.jsx)(e.p,{children:"Planning and reasoning in real-time with complex environments."}),"\n",(0,i.jsx)(e.h3,{id:"uncertainty-management",children:"Uncertainty Management"}),"\n",(0,i.jsx)(e.p,{children:"Dealing with sensor noise, model inaccuracies, and environmental changes."}),"\n",(0,i.jsx)(e.h3,{id:"human-acceptance",children:"Human Acceptance"}),"\n",(0,i.jsx)(e.p,{children:"Ensuring humans trust and accept autonomous robotic systems."}),"\n",(0,i.jsx)(e.h3,{id:"safety-assurance",children:"Safety Assurance"}),"\n",(0,i.jsx)(e.p,{children:"Guaranteeing safe operation in all possible scenarios."}),"\n",(0,i.jsx)(e.h2,{id:"figure-list",children:"Figure List"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Figure 12.1"}),": Autonomous decision-making architecture"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Figure 12.2"}),": Task planning hierarchy"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Figure 12.3"}),": Uncertainty reasoning framework"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Figure 12.4"}),": Human-robot interaction modalities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Figure 12.5"}),": Ethical decision-making process"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"code-example-decision-making-and-autonomy-implementation",children:"Code Example: Decision Making and Autonomy Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport random\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\nfrom datetime import datetime\nimport uuid\n\nclass TaskStatus(Enum):\n    PENDING = "pending"\n    IN_PROGRESS = "in_progress"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\nclass DecisionType(Enum):\n    ACTION = "action"\n    PLANNING = "planning"\n    REASONING = "reasoning"\n    INTERACTION = "interaction"\n\n@dataclass\nclass RobotState:\n    """Current state of the robot"""\n    position: np.ndarray  # [x, y, z] in meters\n    orientation: np.ndarray  # [roll, pitch, yaw] in radians\n    battery_level: float  # 0.0 to 1.0\n    joint_positions: np.ndarray  # Joint angles\n    joint_velocities: np.ndarray  # Joint velocities\n    gripper_state: float  # 0.0 (open) to 1.0 (closed)\n    timestamp: float\n\n@dataclass\nclass Task:\n    """A task for the robot to execute"""\n    id: str\n    name: str\n    description: str\n    priority: int  # 1 (highest) to 10 (lowest)\n    status: TaskStatus\n    dependencies: List[str]  # Other task IDs this task depends on\n    required_resources: List[str]  # Resources needed\n    estimated_duration: float  # in seconds\n    actual_duration: float = 0.0\n    start_time: Optional[float] = None\n    completion_time: Optional[float] = None\n    constraints: Dict[str, Any] = None  # Task-specific constraints\n\n@dataclass\nclass Decision:\n    """A decision made by the robot"""\n    id: str\n    decision_type: DecisionType\n    context: Dict[str, Any]\n    options: List[Dict[str, Any]]\n    selected_option: Dict[str, Any]\n    confidence: float  # 0.0 to 1.0\n    timestamp: float\n    rationale: str\n\nclass UncertaintyModel:\n    """Model for reasoning under uncertainty"""\n\n    def __init__(self):\n        self.sensor_noise = {\n            \'position\': 0.01,  # 1cm standard deviation\n            \'orientation\': 0.017,  # 1 degree standard deviation\n            \'force\': 0.5  # 0.5N standard deviation\n        }\n        self.process_noise = 0.001  # Process model uncertainty\n        self.belief_state = {}  # Current beliefs about world state\n\n    def update_belief(self, sensor_data: Dict[str, float],\n                     action_taken: Optional[str] = None) -> Dict[str, float]:\n        """Update beliefs based on sensor data and actions"""\n        # Simple Kalman filter update for position\n        if \'position\' in sensor_data:\n            current_pos = self.belief_state.get(\'position\', np.array([0.0, 0.0, 0.0]))\n            measurement = np.array(sensor_data[\'position\'])\n\n            # Update with measurement\n            kalman_gain = 0.1  # Simplified constant gain\n            new_pos = current_pos + kalman_gain * (measurement - current_pos)\n\n            self.belief_state[\'position\'] = new_pos\n            self.belief_state[\'position_uncertainty\'] = self.sensor_noise[\'position\'] * (1 - kalman_gain)\n\n        return self.belief_state\n\n    def calculate_uncertainty(self, variable: str) -> float:\n        """Get uncertainty for a specific variable"""\n        return self.belief_state.get(f\'{variable}_uncertainty\', 0.0)\n\n    def sample_from_belief(self, variable: str, num_samples: int = 1) -> np.ndarray:\n        """Sample from belief distribution"""\n        mean = self.belief_state.get(variable, 0.0)\n        uncertainty = self.calculate_uncertainty(variable)\n\n        if isinstance(mean, (int, float)):\n            samples = np.random.normal(mean, uncertainty, num_samples)\n        else:\n            # For arrays\n            samples = np.random.normal(mean, uncertainty, (num_samples,) + mean.shape)\n\n        return samples\n\nclass TaskPlanner:\n    """System for planning and scheduling robot tasks"""\n\n    def __init__(self):\n        self.tasks: List[Task] = []\n        self.active_task: Optional[Task] = None\n        self.completed_tasks: List[Task] = []\n        self.failed_tasks: List[Task] = []\n\n    def add_task(self, task: Task) -> str:\n        """Add a new task to the plan"""\n        self.tasks.append(task)\n        return task.id\n\n    def get_available_tasks(self) -> List[Task]:\n        """Get tasks that are ready to be executed"""\n        available = []\n\n        for task in self.tasks:\n            if task.status == TaskStatus.PENDING:\n                # Check dependencies\n                all_deps_met = True\n                for dep_id in task.dependencies:\n                    dep_task = next((t for t in self.completed_tasks if t.id == dep_id), None)\n                    if dep_task is None:\n                        all_deps_met = False\n                        break\n\n                if all_deps_met:\n                    available.append(task)\n\n        # Sort by priority\n        available.sort(key=lambda t: t.priority)\n        return available\n\n    def prioritize_tasks(self, robot_state: RobotState) -> List[Task]:\n        """Re-prioritize tasks based on current state"""\n        available_tasks = self.get_available_tasks()\n\n        # Adjust priorities based on context\n        for task in available_tasks:\n            # Increase priority for safety-related tasks\n            if \'emergency\' in task.name.lower() or \'safety\' in task.name.lower():\n                task.priority = min(task.priority, 1)\n\n            # Consider resource availability\n            if \'battery\' in task.required_resources and robot_state.battery_level < 0.3:\n                task.priority += 2  # Lower priority if battery is low\n\n        # Sort by priority\n        available_tasks.sort(key=lambda t: t.priority)\n        return available_tasks\n\n    def execute_task(self, task: Task, robot_state: RobotState) -> bool:\n        """Execute a single task"""\n        task.status = TaskStatus.IN_PROGRESS\n        task.start_time = robot_state.timestamp\n\n        # Simulate task execution\n        success = self._simulate_task_execution(task, robot_state)\n\n        task.completion_time = robot_state.timestamp\n        task.actual_duration = task.completion_time - task.start_time if task.start_time else 0\n\n        if success:\n            task.status = TaskStatus.COMPLETED\n            self.completed_tasks.append(task)\n            self.tasks.remove(task)\n            return True\n        else:\n            task.status = TaskStatus.FAILED\n            self.failed_tasks.append(task)\n            self.tasks.remove(task)\n            return False\n\n    def _simulate_task_execution(self, task: Task, robot_state: RobotState) -> bool:\n        """Simulate task execution with some probability of failure"""\n        # Simulate success/failure based on various factors\n        base_success_rate = 0.9\n\n        # Adjust based on task complexity and robot state\n        if task.priority > 5:  # Lower priority tasks might have lower success rate\n            base_success_rate *= 0.95\n\n        if robot_state.battery_level < 0.2:\n            base_success_rate *= 0.8\n\n        return random.random() < base_success_rate\n\nclass HumanRobotInterface:\n    """System for human-robot interaction"""\n\n    def __init__(self):\n        self.conversation_context = {}\n        self.user_preferences = {}\n        self.interaction_history = []\n\n    def process_speech_input(self, speech: str) -> Dict[str, Any]:\n        """Process natural language input"""\n        # Simple keyword-based parsing (in reality, this would use NLP models)\n        tokens = speech.lower().split()\n\n        intent = "unknown"\n        entities = {}\n\n        # Intent recognition\n        if any(word in tokens for word in ["bring", "fetch", "get", "deliver"]):\n            intent = "delivery"\n        elif any(word in tokens for word in ["go", "move", "navigate", "go to"]):\n            intent = "navigation"\n        elif any(word in tokens for word in ["help", "assist", "what", "how"]):\n            intent = "request_info"\n\n        # Entity extraction\n        if "water" in tokens or "drink" in tokens:\n            entities["object"] = "water"\n        elif "cup" in tokens:\n            entities["object"] = "cup"\n\n        if "kitchen" in tokens:\n            entities["location"] = "kitchen"\n        elif "living room" in tokens:\n            entities["location"] = "living_room"\n\n        return {\n            "intent": intent,\n            "entities": entities,\n            "confidence": 0.8  # Simplified confidence\n        }\n\n    def generate_response(self, intent: str, entities: Dict[str, Any]) -> str:\n        """Generate appropriate response to user input"""\n        if intent == "delivery":\n            obj = entities.get("object", "item")\n            return f"OK, I\'ll bring you the {obj}. Where would you like me to deliver it?"\n        elif intent == "navigation":\n            loc = entities.get("location", "destination")\n            return f"OK, I\'ll navigate to the {loc}."\n        elif intent == "request_info":\n            return "I can help you with various tasks. What would you like me to do?"\n        else:\n            return "I\'m not sure I understand. Could you please rephrase?"\n\n    def process_gesture_input(self, gesture_type: str, gesture_data: Dict[str, Any]) -> Dict[str, Any]:\n        """Process gesture input"""\n        # Map gestures to actions\n        gesture_map = {\n            "point_left": "navigate_left",\n            "point_right": "navigate_right",\n            "wave": "greet",\n            "come_here": "approach",\n            "stop": "stop"\n        }\n\n        action = gesture_map.get(gesture_type, "unknown")\n\n        return {\n            "action": action,\n            "gesture_data": gesture_data,\n            "confidence": 0.9\n        }\n\n    def update_user_preferences(self, user_id: str, preferences: Dict[str, Any]):\n        """Update user preferences for personalized interaction"""\n        if user_id not in self.user_preferences:\n            self.user_preferences[user_id] = {}\n\n        self.user_preferences[user_id].update(preferences)\n\nclass DecisionMaker:\n    """System for making autonomous decisions"""\n\n    def __init__(self):\n        self.uncertainty_model = UncertaintyModel()\n        self.decision_history: List[Decision] = []\n        self.ethical_constraints = {\n            \'avoid_harm\': True,\n            \'respect_privacy\': True,\n            \'transparency\': True\n        }\n\n    def make_decision(self, decision_type: DecisionType, context: Dict[str, Any]) -> Decision:\n        """Make a decision based on context and available options"""\n        # Generate options based on decision type\n        options = self._generate_options(decision_type, context)\n\n        # Evaluate options\n        best_option = self._evaluate_options(options, context)\n\n        # Create decision object\n        decision = Decision(\n            id=str(uuid.uuid4()),\n            decision_type=decision_type,\n            context=context,\n            options=options,\n            selected_option=best_option,\n            confidence=best_option.get(\'confidence\', 0.8),\n            timestamp=datetime.now().timestamp(),\n            rationale=best_option.get(\'rationale\', \'Selected based on evaluation criteria\')\n        )\n\n        # Apply ethical constraints\n        if not self._check_ethical_constraints(decision):\n            # Modify decision to comply with constraints\n            decision.selected_option = self._apply_ethical_modifications(decision)\n\n        self.decision_history.append(decision)\n        return decision\n\n    def _generate_options(self, decision_type: DecisionType, context: Dict[str, Any]) -> List[Dict[str, Any]]:\n        """Generate possible options for decision"""\n        options = []\n\n        if decision_type == DecisionType.ACTION:\n            # Generate possible actions\n            robot_pos = context.get(\'robot_position\', np.array([0, 0, 0]))\n            target_pos = context.get(\'target_position\', np.array([1, 1, 0]))\n\n            # Calculate possible actions\n            direction = target_pos - robot_pos\n            distance = np.linalg.norm(direction)\n\n            if distance > 0.1:  # Need to move\n                options.append({\n                    \'action\': \'move_toward\',\n                    \'target\': target_pos,\n                    \'expected_outcome\': f\'Move to {target_pos}\',\n                    \'cost\': distance,\n                    \'confidence\': 0.9\n                })\n\n            options.append({\n                \'action\': \'wait\',\n                \'target\': robot_pos,\n                \'expected_outcome\': \'Remain in current position\',\n                \'cost\': 0,\n                \'confidence\': 0.95\n            })\n\n        elif decision_type == DecisionType.PLANNING:\n            # Generate possible plans\n            tasks = context.get(\'available_tasks\', [])\n\n            for task in tasks[:3]:  # Consider top 3 tasks\n                options.append({\n                    \'plan\': f\'execute_{task.name}\',\n                    \'task_id\': task.id,\n                    \'expected_outcome\': f\'Complete task: {task.name}\',\n                    \'cost\': task.estimated_duration,\n                    \'confidence\': 0.8\n                })\n\n        elif decision_type == DecisionType.REASONING:\n            # Generate reasoning options\n            belief_state = context.get(\'belief_state\', {})\n\n            for var, value in belief_state.items():\n                if isinstance(value, (int, float)) and var.endswith(\'_uncertainty\'):\n                    uncertainty = value\n                    options.append({\n                        \'reasoning\': f\'assess_uncertainty_{var}\',\n                        \'variable\': var.replace(\'_uncertainty\', \'\'),\n                        \'uncertainty_level\': \'high\' if uncertainty > 0.1 else \'low\',\n                        \'confidence\': 0.8\n                    })\n\n        elif decision_type == DecisionType.INTERACTION:\n            # Generate interaction options\n            user_intent = context.get(\'user_intent\', \'unknown\')\n\n            if user_intent == \'delivery\':\n                options.append({\n                    \'interaction\': \'confirm_delivery_request\',\n                    \'expected_outcome\': \'Verify user wants delivery\',\n                    \'confidence\': 0.9\n                })\n                options.append({\n                    \'interaction\': \'request_clarification\',\n                    \'expected_outcome\': \'Ask for more details\',\n                    \'confidence\': 0.85\n                })\n\n        return options\n\n    def _evaluate_options(self, options: List[Dict[str, Any]], context: Dict[str, Any]) -> Dict[str, Any]:\n        """Evaluate and rank options"""\n        if not options:\n            return {}\n\n        # Simple evaluation based on cost and confidence\n        best_option = options[0]\n        best_score = float(\'-inf\')\n\n        for option in options:\n            cost = option.get(\'cost\', 0)\n            confidence = option.get(\'confidence\', 0.5)\n\n            # Calculate score (higher is better)\n            score = confidence - (cost * 0.1)  # Penalize high cost\n\n            if score > best_score:\n                best_score = score\n                best_option = option\n\n        # Add rationale\n        best_option[\'rationale\'] = f"Selected based on score: {best_score:.3f} (confidence: {best_option.get(\'confidence\', 0)}, cost: {best_option.get(\'cost\', 0)})"\n\n        return best_option\n\n    def _check_ethical_constraints(self, decision: Decision) -> bool:\n        """Check if decision violates ethical constraints"""\n        # This is a simplified check - real systems would be more sophisticated\n        selected_action = decision.selected_option.get(\'action\', \'\')\n\n        # Check for potentially harmful actions\n        if \'harm\' in selected_action.lower() or \'damage\' in selected_action.lower():\n            return False\n\n        return True\n\n    def _apply_ethical_modifications(self, decision: Decision) -> Dict[str, Any]:\n        """Modify decision to comply with ethical constraints"""\n        # Return a safe alternative\n        return {\n            \'action\': \'safe_alternative\',\n            \'rationale\': \'Original decision modified for safety\',\n            \'confidence\': 0.9,\n            \'cost\': 0\n        }\n\nclass AutonomySystem:\n    """Main autonomy system coordinating all components"""\n\n    def __init__(self):\n        self.task_planner = TaskPlanner()\n        self.human_interface = HumanRobotInterface()\n        self.decision_maker = DecisionMaker()\n        self.current_state = RobotState(\n            position=np.array([0.0, 0.0, 0.0]),\n            orientation=np.array([0.0, 0.0, 0.0]),\n            battery_level=1.0,\n            joint_positions=np.zeros(20),  # 20 joints example\n            joint_velocities=np.zeros(20),\n            gripper_state=0.0,\n            timestamp=0.0\n        )\n\n        self.autonomy_level = 2  # 0-5 scale (2 = limited autonomy)\n        self.safety_monitoring = True\n\n    def update_state(self, new_state: RobotState):\n        """Update the robot\'s state"""\n        self.current_state = new_state\n\n    def process_user_input(self, input_type: str, input_data: Any) -> str:\n        """Process user input and generate response"""\n        if input_type == "speech":\n            parsed_input = self.human_interface.process_speech_input(input_data)\n            response = self.human_interface.generate_response(\n                parsed_input["intent"],\n                parsed_input["entities"]\n            )\n\n            # Create task based on input if appropriate\n            if parsed_input["intent"] in ["delivery", "navigation"]:\n                task = Task(\n                    id=str(uuid.uuid4()),\n                    name=f"{parsed_input[\'intent\']}_task",\n                    description=f"Perform {parsed_input[\'intent\']} based on user request",\n                    priority=3,\n                    status=TaskStatus.PENDING,\n                    dependencies=[],\n                    required_resources=["navigation", "manipulation"] if parsed_input["intent"] == "delivery" else ["navigation"],\n                    estimated_duration=30.0\n                )\n                self.task_planner.add_task(task)\n\n            return response\n\n        elif input_type == "gesture":\n            gesture_result = self.human_interface.process_gesture_input(input_data["type"], input_data["data"])\n            action = gesture_result["action"]\n\n            # Convert gesture to appropriate action\n            if action == "approach":\n                # Create navigation task to approach user\n                task = Task(\n                    id=str(uuid.uuid4()),\n                    name="approach_user",\n                    description="Navigate to user location",\n                    priority=2,\n                    status=TaskStatus.PENDING,\n                    dependencies=[],\n                    required_resources=["navigation"],\n                    estimated_duration=15.0\n                )\n                self.task_planner.add_task(task)\n                return "Approaching you now."\n\n            return f"Gesture recognized: {action}"\n\n        return "Input not recognized"\n\n    def autonomous_decision_cycle(self) -> List[Decision]:\n        """Main decision-making cycle"""\n        decisions = []\n\n        # Get available tasks\n        available_tasks = self.task_planner.prioritize_tasks(self.current_state)\n\n        if available_tasks:\n            # Make decision about which task to execute\n            context = {\n                \'available_tasks\': available_tasks,\n                \'robot_state\': self.current_state,\n                \'belief_state\': self.decision_maker.uncertainty_model.belief_state\n            }\n\n            decision = self.decision_maker.make_decision(DecisionType.PLANNING, context)\n            decisions.append(decision)\n\n            # Execute selected task\n            selected_task_id = decision.selected_option.get(\'task_id\')\n            if selected_task_id:\n                task = next((t for t in available_tasks if t.id == selected_task_id), None)\n                if task:\n                    success = self.task_planner.execute_task(task, self.current_state)\n                    if not success:\n                        print(f"Task {task.name} failed")\n\n        # Make other autonomous decisions as needed\n        if self.current_state.battery_level < 0.2:\n            # Decision to return to charging station\n            context = {\n                \'battery_level\': self.current_state.battery_level,\n                \'charging_station_pos\': np.array([0, 0, 0])\n            }\n            decision = self.decision_maker.make_decision(DecisionType.ACTION, context)\n            decisions.append(decision)\n\n        return decisions\n\n    def get_system_status(self) -> Dict[str, Any]:\n        """Get current system status"""\n        return {\n            \'autonomy_level\': self.autonomy_level,\n            \'battery_level\': self.current_state.battery_level,\n            \'active_tasks\': len([t for t in self.task_planner.tasks if t.status == TaskStatus.IN_PROGRESS]),\n            \'pending_tasks\': len([t for t in self.task_planner.tasks if t.status == TaskStatus.PENDING]),\n            \'completed_tasks\': len(self.task_planner.completed_tasks),\n            \'failed_tasks\': len(self.task_planner.failed_tasks),\n            \'decision_count\': len(self.decision_maker.decision_history),\n            \'safety_monitoring\': self.safety_monitoring\n        }\n\ndef demonstrate_autonomy_system():\n    """Demonstrate autonomous decision making concepts"""\n    print("Decision Making and Autonomy - Chapter 12")\n    print("=" * 50)\n\n    # Initialize autonomy system\n    autonomy_system = AutonomySystem()\n\n    print("1. Task Planning Demo:")\n\n    # Create some sample tasks\n    tasks = [\n        Task(\n            id=str(uuid.uuid4()),\n            name="navigation_to_kitchen",\n            description="Navigate to kitchen area",\n            priority=2,\n            status=TaskStatus.PENDING,\n            dependencies=[],\n            required_resources=["navigation"],\n            estimated_duration=20.0\n        ),\n        Task(\n            id=str(uuid.uuid4()),\n            name="object_identification",\n            description="Identify objects in kitchen",\n            priority=3,\n            status=TaskStatus.PENDING,\n            dependencies=["navigation_to_kitchen"],\n            required_resources=["vision"],\n            estimated_duration=10.0\n        ),\n        Task(\n            id=str(uuid.uuid4()),\n            name="grasp_object",\n            description="Grasp identified object",\n            priority=4,\n            status=TaskStatus.PENDING,\n            dependencies=["object_identification"],\n            required_resources=["manipulation"],\n            estimated_duration=15.0\n        )\n    ]\n\n    for task in tasks:\n        autonomy_system.task_planner.add_task(task)\n\n    print(f"   - Added {len(tasks)} tasks to planner")\n    print(f"   - Available tasks: {[t.name for t in autonomy_system.task_planner.get_available_tasks()]}")\n\n    print("\\n2. Human-Robot Interaction Demo:")\n\n    # Simulate user input\n    speech_input = "Can you bring me a cup of water from the kitchen?"\n    response = autonomy_system.process_user_input("speech", speech_input)\n    print(f"   - User: \'{speech_input}\'")\n    print(f"   - Robot: \'{response}\'")\n\n    # Simulate gesture input\n    gesture_input = {"type": "wave", "data": {"position": [1, 0, 0]}}\n    gesture_response = autonomy_system.process_user_input("gesture", gesture_input)\n    print(f"   - Gesture detected: wave")\n    print(f"   - Robot response: \'{gesture_response}\'")\n\n    print("\\n3. Autonomous Decision Making Demo:")\n\n    # Simulate robot state updates\n    new_state = RobotState(\n        position=np.array([1.0, 0.5, 0.0]),\n        orientation=np.array([0.0, 0.0, 0.1]),\n        battery_level=0.75,\n        joint_positions=np.random.random(20) * 0.5,\n        joint_velocities=np.random.random(20) * 0.1,\n        gripper_state=0.0,\n        timestamp=time.time()\n    )\n    autonomy_system.update_state(new_state)\n\n    # Run autonomous decision cycle\n    decisions = autonomy_system.autonomous_decision_cycle()\n    print(f"   - Made {len(decisions)} autonomous decisions:")\n    for i, decision in enumerate(decisions[:2]):  # Show first 2 decisions\n        print(f"     Decision {i+1}: {decision.decision_type.value} - {decision.selected_option.get(\'action\', \'N/A\')}")\n\n    print("\\n4. Uncertainty Reasoning Demo:")\n\n    # Simulate sensor data with uncertainty\n    sensor_data = {\n        \'position\': [1.05, 0.48, 0.02],  # Noisy position measurement\n        \'battery_level\': 0.74  # Slightly different from reported\n    }\n\n    updated_beliefs = autonomy_system.decision_maker.uncertainty_model.update_belief(sensor_data)\n    print(f"   - Updated beliefs: position={updated_beliefs.get(\'position\', \'N/A\')}")\n    print(f"   - Position uncertainty: {updated_beliefs.get(\'position_uncertainty\', \'N/A\')}")\n\n    # Sample from belief distribution\n    position_samples = autonomy_system.decision_maker.uncertainty_model.sample_from_belief(\'position\', 5)\n    print(f"   - Sampled positions: {position_samples[:2]}...")  # Show first 2 samples\n\n    print("\\n5. System Status:")\n    status = autonomy_system.get_system_status()\n    for key, value in status.items():\n        print(f"   - {key.replace(\'_\', \' \').title()}: {value}")\n\n    print("\\n6. Ethical Decision Making:")\n\n    # Demonstrate ethical constraint checking\n    test_decision = Decision(\n        id=str(uuid.uuid4()),\n        decision_type=DecisionType.ACTION,\n        context={\'test\': True},\n        options=[{\'action\': \'move_toward_object\'}],\n        selected_option={\'action\': \'move_toward_object\'},\n        confidence=0.9,\n        timestamp=time.time(),\n        rationale="Test decision"\n    )\n\n    ethical_check = autonomy_system.decision_maker._check_ethical_constraints(test_decision)\n    print(f"   - Ethical constraint check passed: {ethical_check}")\n\n    print("\\n7. Autonomy Levels:")\n    autonomy_levels = {\n        0: "No autonomy - fully human controlled",\n        1: "Low autonomy - human gives simple commands",\n        2: "Limited autonomy - robot handles basic tasks",\n        3: "Conditional autonomy - robot handles most tasks with human oversight",\n        4: "High autonomy - robot handles complex tasks independently",\n        5: "Full autonomy - robot handles all tasks without human intervention"\n    }\n\n    current_level = autonomy_system.autonomy_level\n    print(f"   - Current autonomy level: {current_level}")\n    print(f"   - Description: {autonomy_levels[current_level]}")\n\n    # Performance metrics\n    print("\\n8. Performance Analysis:")\n    print(f"   - Decision making speed: {len(decisions)/0.1:.1f} decisions/second (simulated)")\n    print(f"   - Task success rate: {len(autonomy_system.task_planner.completed_tasks)}/{len(autonomy_system.task_planner.completed_tasks) + len(autonomy_system.task_planner.failed_tasks) or 1:.1%}")\n    print(f"   - Current battery level: {autonomy_system.current_state.battery_level:.1%}")\n\n    return {\n        \'decisions_made\': len(decisions),\n        \'tasks_completed\': len(autonomy_system.task_planner.completed_tasks),\n        \'current_battery\': autonomy_system.current_state.battery_level,\n        \'system_status\': status\n    }\n\ndef analyze_decision_performance(results: Dict) -> Dict:\n    """Analyze decision making performance metrics"""\n    analysis = {\n        \'decision_quality\': {\n            \'decisions_per_cycle\': results[\'decisions_made\'],\n            \'decision_diversity\': \'N/A\'  # Would need more detailed tracking\n        },\n        \'task_performance\': {\n            \'tasks_completed\': results[\'tasks_completed\'],\n            \'efficiency\': results[\'tasks_completed\'] / max(results[\'decisions_made\'], 1)\n        },\n        \'system_health\': {\n            \'battery_level\': results[\'current_battery\'],\n            \'system_stability\': \'Good\'  # Would be determined by detailed monitoring\n        },\n        \'interaction_quality\': {\n            \'responses_generated\': 2,  # From the demo\n            \'understanding_accuracy\': \'N/A\'  # Would require evaluation\n        }\n    }\n\n    return analysis\n\ndef discuss_ethical_considerations():\n    """Discuss ethical considerations in autonomous robotics"""\n    print(f"\\n9. Ethical Considerations in Autonomous Robotics:")\n\n    ethical_principles = [\n        ("Beneficence", "Act in ways that benefit humans and promote wellbeing"),\n        ("Non-maleficence", "Avoid causing harm to humans or property"),\n        ("Autonomy", "Respect human autonomy and decision-making"),\n        ("Justice", "Ensure fair treatment regardless of user characteristics"),\n        ("Explainability", "Provide clear explanations for robot decisions"),\n        ("Privacy", "Protect user data and maintain confidentiality")\n    ]\n\n    print("\\n   Core Ethical Principles:")\n    for principle, description in ethical_principles:\n        print(f"     - {principle}: {description}")\n\n    implementation_strategies = [\n        "Value-sensitive design: Incorporate ethical values during system design",\n        "Fail-safe mechanisms: Ensure safe behavior when systems fail",\n        "Human oversight: Maintain human control over critical decisions",\n        "Transparency: Make system capabilities and limitations clear",\n        "Consent mechanisms: Obtain permission for data collection and use"\n    ]\n\n    print("\\n   Implementation Strategies:")\n    for strategy in implementation_strategies:\n        print(f"     - {strategy}")\n\n    challenges = [\n        "Balancing autonomy with safety",\n        "Handling conflicting ethical principles",\n        "Ensuring fairness across diverse populations",\n        "Maintaining privacy while providing personalized service",\n        "Dealing with unpredictable human behavior"\n    ]\n\n    print("\\n   Key Challenges:")\n    for challenge in challenges:\n        print(f"     - {challenge}")\n\nif __name__ == "__main__":\n    import time  # Import time for timestamp operations\n\n    # Run the demonstration\n    results = demonstrate_autonomy_system()\n\n    # Analyze performance\n    performance_analysis = analyze_decision_performance(results)\n\n    print(f"\\n10. Performance Analysis Summary:")\n    for category, metrics in performance_analysis.items():\n        print(f"\\n   {category.replace(\'_\', \' \').title()}:")\n        for metric, value in metrics.items():\n            print(f"     - {metric.replace(\'_\', \' \')}: {value}")\n\n    # Discuss ethical considerations\n    discuss_ethical_considerations()\n\n    print(f"\\n11. Key Takeaways:")\n    print("    - Autonomous systems require sophisticated planning and reasoning")\n    print("    - Uncertainty management is crucial for real-world operation")\n    print("    - Human-robot interaction needs to be natural and intuitive")\n    print("    - Ethical considerations must be built into system design")\n    print("    - Safety and reliability are paramount in autonomous systems")\n\n    print(f"\\nDecision Making and Autonomy - Chapter 12 Complete!")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Implement a task planner that can handle temporal and resource constraints for a multi-step manipulation task."}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Design a decision-making system that incorporates ethical constraints for a service robot operating in a hospital environment."}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Create a human-robot interaction system that can adapt its communication style based on user preferences and context."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary-1",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter provided a comprehensive overview of decision-making and autonomy systems for humanoid robots, covering task planning, reasoning under uncertainty, human-robot interaction, and ethical considerations. We explored mathematical foundations, practical implementations, and the challenges of creating truly autonomous robotic systems. The concepts and code examples presented will help in developing intelligent robots that can operate independently while maintaining safety and ethical standards."})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);